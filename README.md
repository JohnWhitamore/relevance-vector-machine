# relevance-vector-machine

### What is the Relevance Vector Machine?

This repo implements sparse Bayesian regression in the form of the Relevance Vector Machine (RVM) introduced by Tipping (2001) [Sparse Bayesian Learning and the Relevance Vector Machine](https://jmlr.csail.mit.edu/papers/v1/tipping01a.html).

The model writes a Gaussian prior over regression weights that has an anisotropic axis-aligned covariance. Allowing each basis function to have its own variance provides a mechanism for pruning irrelevant basis functions from the model. The mechanism is, on each iteration of the EM algorithm, to identify basis functions whose precision is very large and whose mean is very small. These basis functions have posterior weights that - with a high degree of certainty - are zero and can be removed from the model.

It is useful to understand that the mechanism described is not simply a heuristic. Removing basis functions leads to a covariance matrix with smaller dimensions and (typically) a smaller determinant. The determinant is used in the normalisation term of the marginal likelihood calculation. The model balances model complexity (quantified by the determinant of the covariance) against accuracy (quantified by the likelihood variance) in order to maximise marginal likelihood.

The approach is extended (and reversed) by Tipping and Faul (2003) [Fast Marginal Likelihood Maximisation for Sparse Bayesian Models](https://proceedings.mlr.press/r4/tipping03a.html). The Fast RVM initialises itself with just one basis function and progressively seeks to add (or swap) further basis functions until doing so proves fruitless or - more formally - ceases to increase the marginal likelihood. In this model, the number of basis functions $M$ starts (and tends to remain) low, which leads to computational efficiencies from faster $O(M^{3})$ matrix inversion.

### Why is sparsity important?

Basis functions are mappings from data that encode hypotheses about the drivers of observed data. The circumstance of having a multitude of hypotheses about a data generation process is extremely widespread in science, the humanities, finance and commerce. The widespread applicability of sparse models makes the study of sparsity central to any serious attempt to understand the world around us.

### src/

The source code includes modules for:

1. `xdata.py`: creating a design matrix
2. `synthdata.py`: creating synthetic data which provide a ground truth against which model results can be tested
3. `em.py`: runs the EM algorithm with the addition of a mechanism for pruning basis functions from the model to encode sparsity
4. `main.py`: calls the code in the other modules

### Example results

In each case, the green dots represent observed data, the blue line is the ground truth generative mean and the black line is the mean learned by the model. 

All results were generated by the same model without alteration or pre-calibration.

1. The model detects that the data is simply bias plus noise and prunes away all other basis functions
<img width="568" height="413" alt="bias_only" src="https://github.com/user-attachments/assets/61e2bf35-7207-4d0c-8c0e-4cf464d031ef" />

2. The model detects a bias and a simple seasonal pattern
<img width="568" height="414" alt="seasons" src="https://github.com/user-attachments/assets/2d7423e1-4618-45e7-a5b6-a6790efb7476" />

3. The model detects a more complex generating process
<img width="568" height="413" alt="more_complex" src="https://github.com/user-attachments/assets/36a938e6-623d-4c8b-8cbe-8f0584e5c754" />





